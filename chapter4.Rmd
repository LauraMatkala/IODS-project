# Clustering and classification

### The data

This weeks data needs no specific wrangling beforehand, as it comes directly from a package "MASS". We are using a dataset called Boston, which has variables related to housing in the suburbs of Boston, Massachusettes, USA. The data includes the following columns:


* crim = per capita crime rate by town.
* zn = proportion of residential land zoned for lots over 25,000 sq.ft.
* indus = proportion of non-retail business acres per town.
* chas = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
* nox = nitrogen oxides concentration (parts per 10 million).
* rm = average number of rooms per dwelling.
* age = proportion of owner-occupied units built prior to 1940.
* dis = weighted mean of distances to five Boston employment centres.
* rad = index of accessibility to radial highways.
* tax = full-value property-tax rate per \$10,000.
* ptratio = pupil-teacher ratio by town.
* black = 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
* lstat = lower status of the population (percent).
* medv = median value of owner-occupied homes in \$1000s.

As we can see, there is diverse data related to e.g. the crime rate, air quality (nox) and the proportion of black citizens.

I will explore the data set a bit:

```{r Boston}
library(MASS)
data("Boston")
str(Boston)
dim(Boston)
summary(Boston)
library(corrplot); library(tidyverse);library(ggplot2); library(GGally)
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
cor_matrix<-cor(Boston)
cor_matrix %>% round(digits =2)
corrplot(cor_matrix, method="circle", type ="upper", cl.pos = "b", tl.pos ="d", tl.cex = 0.6)
```

The dataset consists of 506 rows and 14 columns. Most variables are numeric, except for "chas" and "rad", which are integers. With this many variables the pairs- plot looks really messy, so I decided to plot a bar plot and a correlation plot. The bar plot is not really working either, because some varibales, such as "black" or "dis" can hardly be seen at all in the graph. According to the correlation plot variable "crim" is best correlated (positively) with variables "rad" and "tax". Overall the best positive correlation is between "indus" and "nox" and best negative correlations between "nox" and "age", "age" and "dis", "indus" and "dis" as well as "lstat" and "medv".

### Dataset wrangling

I will next standardize the dataset and print out the summaries to see what standardizing did to the dataset. I will also create a categorical variable of the (scaled) crime rate, as well as replace the old "crim" with the new "crime" adn split the dataset into train and test sets.

```{r standardize}
boston_scaled <- scale(Boston)
summary(boston_scaled)
class(boston_scaled)
boston_scaled<-as.data.frame(boston_scaled)
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low", "med_low", "med_high", "high"))
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```

I see that after scaling the values are now much smaller than they were in the non-scaled dataset, often even negative medians and minimum values. 

### Linear discriminant analysis

I will next perform a linear discriminant analysis on the dataset "train". 

```{r ldr}
lda.fit <- lda(crime~., data = train)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2.3)
```

The plot shows also that with high crime rate variable "rad" is higher. Just to remind ourselves, "rad" equals to index of accessibility to radial highways. I guess it means that with higher index there are better possibilities to access the highway, and thus escape after committing a crime...

I will now save the crime categories and remove the categorical crime variable from the "test" dataset.

```{r replace2}
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
```

Now I use my LDA model for predicting the classes with the test data and do crosstabulation of the predicted and actual classes from the test data.

```{r predict}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```

For high quantile the prediction seems to fit with correct values perfectly, as all of the cases could be predicted. All in all, the model is predicting actual cases quite well. 

### k-means

Let's reload the Boston dataset and scale it for some k-means calculations.

```{r reload}
library(MASS)
data(Boston)
boston_scaled <- scale(Boston)
class(boston_scaled)
boston_scaled<-as.data.frame(boston_scaled)
```

I want to calculate the distances between observations and will do that next. I will use Euclidian distances.

```{r distance}

dist_eu <- dist(boston_scaled)
summary(dist_eu)
```


Now some k-means clustering:

```{r kmeans}
km <-kmeans(boston_scaled, centers = 4)
pairs(boston_scaled, col = km$cluster) 


```

The results seem to be similar than they were with LDA, although the figure is rather messy and partially difficult to interpret.

I will investigate what is the optimal number of clusters. The optimal number of clusters is when the total WCSS (within cluster sum of squares) drops radically. 

```{r optimal}
set.seed(123)
k_max <- 10 #  
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

Looking at the figure above it seems that the optimum amount of clusters is less than 2, since that is where WCSS drops down most radically. 



